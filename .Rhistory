data_temp <- cqpmData$c5
data_temp <- data.frame(cbind(data_temp,y))
# Split data to train and test (uesing caTools lib).
set.seed(101)
sample = sample.split(data_temp$y, SplitRatio = .75)
train = subset(data_temp, sample == TRUE)
test = subset(data_temp, sample == FALSE)
modelID3 <- ID3(test,'y')
modelID3 <- ID3(train,'y')
pID3 <- predict_ID3(test,modelID3)
pID3 <- predict_ID3(train,modelID3)
pID3 <- predict_ID3(test,modelID3)
pID3
pID3 <- null
pID3 <- NULL
pID3 <- predict_ID3(test,modelID3)
pID3
= "class")
pRPART <- predict(modelRPART, test, type = "class")
modelRPART <- rpart(y~., data=train, method = "class")
pRPART <- predict(modelRPART, test, type = "class")
table(Actual=test$y, Fitted=pRPART)
pID3 <- predict_ID3(test,modelID3)
modelID3 <- ID3(train[1:392,],'y')
pID3 <- predict_ID3(test,modelID3)
pID3 <- predict_ID3(test,modelID3)
length(train)
length(test)
modelID3 <- ID3(train,'y')
pID3 <- predict_ID3(train[1:2,], modelID3)
pID3
View(test)
View(test)
View(train)
typeof(test)
typeof(train)
test[1,]
test
pID3 <- predict_ID3(train[1:2,], modelID3)
pID3 <- predict_ID3(test[1,2,], modelID3)
pID3 <- predict_ID3(test[1:2,], modelID3)
View(test)
View(train)
pID3 <- predict_ID3(test[1,],modelID3)
pID3 <- predict_ID3(test[2,],modelID3)
pID3 <- predict_ID3(test[3,],modelID3)
train[1,]
train[1175,]
pID3 <- predict_ID3(data_temp,modelID3)
pID3 <- predict_ID3(train,modelID3)
IsPure <- function(data) {
length(unique(data[,ncol(data)])) == 1
}
Entropy <- function( vls ) {
res <- vls/sum(vls) * log2(vls/sum(vls))
res[vls == 0] <- 0
-sum(res)
}
InformationGain <- function( tble ) {
entropyBefore <- Entropy(colSums(tble))
s <- rowSums(tble)
entropyAfter <- sum (s / sum(s) * apply(tble, MARGIN = 1, FUN = Entropy ))
informationGain <- entropyBefore - entropyAfter
return (informationGain)
}
TrainID3 <- function(node, data) {
node$obsCount <- nrow(data)
#if the data-set is pure (e.g. all toxic), then
if (IsPure(data)) {
#construct a leaf having the name of the pure feature (e.g. 'toxic')
child <- node$AddChild(unique(data[,ncol(data)]))
node$feature <- tail(names(data), 1)
child$obsCount <- nrow(data)
child$feature <- ''
} else {
#calculate the information gain
ig <- sapply(colnames(data)[-ncol(data)],
function(x) InformationGain(
table(data[,x], data[,ncol(data)])
)
)
#chose the feature with the highest information gain (e.g. 'color')
#if more than one feature have the same information gain, then take
#the first one
feature <- names(which.max(ig))
node$feature <- feature
#take the subset of the data-set having that feature value
childObs <- split(data[ ,names(data) != feature, drop = FALSE],
data[ ,feature],
drop = TRUE)
for(i in 1:length(childObs)) {
#construct a child having the name of that feature value (e.g. 'red')
child <- node$AddChild(names(childObs)[i])
#call the algorithm recursively on the child and the subset
TrainID3(child, childObs[[i]])
}
}
}
treePredict <- function(tree, features) {
if (tree$children[[1]]$isLeaf) return (tree$children[[1]]$name)
child <- tree$children[[features[[tree$feature]]]]
return ( treePredict(child, features))
}
library(data.tree)
data(mushroom)
mushroom
tree <- Node$new("mushroom")
TrainID3(tree, mushroom)
print(tree, "feature", "obsCount")
treePredict(tree, c(color = 'red',
size = 'large',
points = 'yes')
)
data <- train[,20:23]
tree <- Node$new(data)
data <- train[1:100,20:23]
tree <- Node$new(data)
data <- train[1:5,20:23]
tree <- Node$new(data)
data <- round(data, digits = 5)
View(data)
tree <- Node$new(data)
View(data)
colnames(data) <- c("x1","x2","x3","y")
View(data)
tree <- Node$new(data)
data *10
data <- data * 100
tree <- Node$new("mushroom")
tree <- Node$new(data)
typeof(mushroom)
mushroom
tree <- Node$new(data)
View(data)
colnames(data) <- c("color","size","points","edibility")
tree <- Node$new(data)
View(data)
tree
TrainID3(tree, data.frame(data))
tree
treePredict(tree, c(Comp.20 = -2.798285e-05,
Comp.21 = 7.681122e-04,
Comp.22 = 8.403139e-04)
)
test[1,20:22] * 100
round(test[1,20:22], digits = 5) * 100
treePredict(tree, c(Comp.20 = -0.03,
Comp.21 = 0.077,
Comp.22 = 0.084)
)
print(tree, "feature", "obsCount")
tree <- Node$new("data")
TrainID3(tree, data.frame(data))
print(tree, "feature", "obsCount")
treePredict(tree, c(Comp.20 = -0.03,
Comp.21 = 0.077,
Comp.22 = 0.084)
)
library(data.tree)
data <-train
tree <- Node$new("data")
TrainID3(tree, data)
print(tree, "feature", "obsCount")
treePredict(tree, test[,1:22])
treePredict(tree, test[1,1:22])
summary(tree)
treePredict(tree, c(Comp.20 = -0.03, Comp.21 = 0.077, Comp.22 = 0.084))
print(tree)
plot(tree)
print(tree, "feature", "obsCount", "b")
train
library(data.tree)
data <-train
tree <- Node$new("data")
TrainID3(tree, data)
tree
print(tree, "feature", "obsCount")
library(data.tree)
data(mushroom)
mushroom
tree <- Node$new("mushroom")
TrainID3(tree, mushroom)
print(tree, "feature", "obsCount")
library(data.tree)
data <-train
tree <- Node$new("data")
TrainID3(tree, data)
print(tree, "feature", "obsCount")
data <-train
tree <- Node$new("data")
node <- tree
node$obsCount <- nrow(data)
#if the data-set is pure (e.g. all toxic), then
if (IsPure(data)) {
#construct a leaf having the name of the pure feature (e.g. 'toxic')
child <- node$AddChild(unique(data[,ncol(data)]))
node$feature <- tail(names(data), 1)
child$obsCount <- nrow(data)
child$feature <- ''
} else {
#calculate the information gain
ig <- sapply(colnames(data)[-ncol(data)],
function(x) InformationGain(
table(data[,x], data[,ncol(data)])
)
)
#chose the feature with the highest information gain (e.g. 'color')
#if more than one feature have the same information gain, then take
#the first one
feature <- names(which.max(ig))
node$feature <- feature
#take the subset of the data-set having that feature value
childObs <- split(data[ ,names(data) != feature, drop = FALSE],
data[ ,feature],
drop = TRUE)
for(i in 1:length(childObs)) {
#construct a child having the name of that feature value (e.g. 'red')
child <- node$AddChild(names(childObs)[i])
#call the algorithm recursively on the child and the subset
TrainID3(child, childObs[[i]])
}
}
node
plot(node)
library(data.tree)
data <-train
tree <- Node$new("data")
TrainID3(tree, data)
print(tree, "feature", "obsCount")
treePredict(tree, test[,1:22])
pID3 <- predict_ID3(train[-1,],modelID3)
pID3 <- predict_ID3(test,modelID3)
modelID3 <- ID3(train[-1,],'y')
as.numeric(secom$y)
is.numeric(secom$y)
is.factor(secom$y)
typeof(secom$y[1])
secom$y[1]
secom$y[1,]
secom$y
secom
is.factor(secom$LABEL)
is.numeric(secom$LABEL)
modelID3 <- ID3(train,'y')
setwd("/Volumes/SANDISK_III/project/R/CQPMpca_id3/qrm")
# Loading function and library
source("resource.R")
loadSources()
# Subset the secom dataset to only 2 labels
secom <- read.csv("dataset/secom/secom.csv")
secom <- preProcessing(secom)
x <- subset(secom, select = -LABEL)
y <- secom$LABEL
# Feature selection.
x <- fs.ridge(x,y,40)
# fs.svm.x <- fs.svm(secom, y)
# Modeling.
cqpmData <- pca.cqpm(x)
data_temp <- cqpmData$c5
data_temp <- data.frame(cbind(data_temp,y))
# Split data to train and test (uesing caTools lib).
set.seed(101)
sample = sample.split(data_temp$y, SplitRatio = .75)
train = subset(data_temp, sample == TRUE)
test = subset(data_temp, sample == FALSE)
require(ape)
foo <- read.tree(tree)
tree
library(data.tree)
data <-train
tree <- Node$new("data")
TrainID3(tree, data)
print(tree, "feature", "obsCount")
IsPure <- function(data) {
length(unique(data[,ncol(data)])) == 1
}
Entropy <- function( vls ) {
res <- vls/sum(vls) * log2(vls/sum(vls))
res[vls == 0] <- 0
-sum(res)
}
InformationGain <- function( tble ) {
entropyBefore <- Entropy(colSums(tble))
s <- rowSums(tble)
entropyAfter <- sum (s / sum(s) * apply(tble, MARGIN = 1, FUN = Entropy ))
informationGain <- entropyBefore - entropyAfter
return (informationGain)
}
TrainID3 <- function(node, data) {
node$obsCount <- nrow(data)
#if the data-set is pure (e.g. all toxic), then
if (IsPure(data)) {
#construct a leaf having the name of the pure feature (e.g. 'toxic')
child <- node$AddChild(unique(data[,ncol(data)]))
node$feature <- tail(names(data), 1)
child$obsCount <- nrow(data)
child$feature <- ''
} else {
#calculate the information gain
ig <- sapply(colnames(data)[-ncol(data)],
function(x) InformationGain(
table(data[,x], data[,ncol(data)])
)
)
#chose the feature with the highest information gain (e.g. 'color')
#if more than one feature have the same information gain, then take
#the first one
feature <- names(which.max(ig))
node$feature <- feature
#take the subset of the data-set having that feature value
childObs <- split(data[ ,names(data) != feature, drop = FALSE],
data[ ,feature],
drop = TRUE)
for(i in 1:length(childObs)) {
#construct a child having the name of that feature value (e.g. 'red')
child <- node$AddChild(names(childObs)[i])
#call the algorithm recursively on the child and the subset
TrainID3(child, childObs[[i]])
}
}
}
treePredict <- function(tree, features) {
if (tree$children[[1]]$isLeaf) return (tree$children[[1]]$name)
child <- tree$children[[features[[tree$feature]]]]
return ( treePredict(child, features))
}
library(data.tree)
data <-train
tree <- Node$new("data")
TrainID3(tree, data)
print(tree, "feature", "obsCount")
foo <- read.tree(tree)
tree
library(data.tree)
data(mushroom)
mushroom
tree <- Node$new("mushroom")
TrainID3(tree, mushroom)
print(tree, "feature", "obsCount")
tree
library(data.tree)
data <-train
tree <- Node$new("data")
TrainID3(tree, data)
tree$`0.000101781186413878`
library(data.tree)
data(mushroom)
mushroom
tree <- Node$new("mushroom")
TrainID3(tree, mushroom)
tree$count
tree <- Node$new("mushroom")
tree <- Node$new("data")
TrainID3(tree, data)
tree$.__enclos_env__
tree <- Node$new("mushroom")
TrainID3(tree, mushroom)
tree$children
tree <- Node$new("data")
TrainID3(tree, data)
tree$children
abc <- tree$children
abc
abc$`-2.14190112936774e-05`
summary(abc)
data <- as.data.frame(rbind(
list("sunny"   ,      85     ,    85    , "windy" , "Don't Play"),
list("sunny"   ,      80     ,    90    , "wind"  , "Don't Play"),
list("overcast",      83     ,    78    , 'calm' , "Play"),
list("rain"    ,      70     ,    96    , 'calm' , "Play"),
list("rain"    ,      68     ,    80    , 'calm' , "Play"),
list("rain"    ,      65     ,    70    , "wind"  , "Don't Play"),
list("overcast",      64     ,    65    , "wind"  , "Play"),
list("sunny"   ,      72     ,    95    , 'calm' , "Don't Play"),
list("sunny"   ,      69     ,    70    , 'calm' , "Play"),
list("rain"    ,      75     ,    80    , 'calm' , "Play"),
list("sunny"   ,      75     ,    70    , "wind"  , "Play"),
list("overcast",      72     ,    90    , "wind"  , "Play"),
list("overcast",      81     ,    75    , 'calm' , "Play"),
list("rain"    ,      71     ,    80    , "wind"  , "Don't Play")))
names(data) <- c('outlook', 'temperature', 'humidity', 'windy', 'play')
data$temperature <- as.numeric(data$temperature)
data$humidity <- as.numeric(data$humidity)
data$windy <- as.factor(unlist(data$windy))
data$outlook <- as.factor(unlist(data$outlook))
data$play <- as.factor(unlist(data$play))
data$temperature <- cut(data$temperature, breaks = c(0, 75, max(data$temperature)))
data$humidity <- cut(data$humidity, breaks = c(0, 75, max(data$humidity)))
decision.tree <- ID3(data[-1, ], 'play')
predict_ID3(data[1,], decision.tree)
decision.tree <- ID3(data[-1, ], 'play')
setwd("/Volumes/SANDISK_III/project/R/CQPMpca_id3/qrm")
# Loading function and library
source("resource.R")
loadSources()
# Subset the secom dataset to only 2 labels
secom <- read.csv("dataset/secom/secom.csv")
secom <- preProcessing(secom)
x <- subset(secom, select = -LABEL)
y <- secom$LABEL
# Feature selection.
x <- fs.ridge(x,y,40)
# fs.svm.x <- fs.svm(secom, y)
# Modeling.
cqpmData <- pca.cqpm(x)
data_temp <- cqpmData$c5
data_temp <- data.frame(cbind(data_temp,y))
# Split data to train and test (uesing caTools lib).
set.seed(101)
sample = sample.split(data_temp$y, SplitRatio = .75)
train = subset(data_temp, sample == TRUE)
test = subset(data_temp, sample == FALSE)
seample
example
data <- as.data.frame(rbind(
list("sunny"   ,      85     ,    85    , "windy" , "Don't Play"),
list("sunny"   ,      80     ,    90    , "wind"  , "Don't Play"),
list("overcast",      83     ,    78    , 'calm' , "Play"),
list("rain"    ,      70     ,    96    , 'calm' , "Play"),
list("rain"    ,      68     ,    80    , 'calm' , "Play"),
list("rain"    ,      65     ,    70    , "wind"  , "Don't Play"),
list("overcast",      64     ,    65    , "wind"  , "Play"),
list("sunny"   ,      72     ,    95    , 'calm' , "Don't Play"),
list("sunny"   ,      69     ,    70    , 'calm' , "Play"),
list("rain"    ,      75     ,    80    , 'calm' , "Play"),
list("sunny"   ,      75     ,    70    , "wind"  , "Play"),
list("overcast",      72     ,    90    , "wind"  , "Play"),
list("overcast",      81     ,    75    , 'calm' , "Play"),
list("rain"    ,      71     ,    80    , "wind"  , "Don't Play")))
names(data) <- c('outlook', 'temperature', 'humidity', 'windy', 'play')
data$temperature <- as.numeric(data$temperature)
data$humidity <- as.numeric(data$humidity)
data$windy <- as.factor(unlist(data$windy))
data$outlook <- as.factor(unlist(data$outlook))
data$play <- as.factor(unlist(data$play))
data$temperature <- cut(data$temperature, breaks = c(0, 75, max(data$temperature)))
data$humidity <- cut(data$humidity, breaks = c(0, 75, max(data$humidity)))
decision.tree <- ID3(data[-1, ], 'play')
predict_ID3(data[1,], decision.tree)
data
data[-1,]
modelID3 <- ID3(train,'y')
pID3 <- predict_ID3(test[1,],modelID3)
round(train, digits = 6)
train <- round(train, digits = 6)
test <- round(test, digits = 6)
modelID3 <- ID3(train,'y')
pID3 <- predict_ID3(test[1,],modelID3)
summary(modelID3)
data <- as.data.frame(rbind(
list("sunny"   ,      85     ,    85    , "windy" , "Don't Play"),
list("sunny"   ,      80     ,    90    , "wind"  , "Don't Play"),
list("overcast",      83     ,    78    , 'calm' , "Play"),
list("rain"    ,      70     ,    96    , 'calm' , "Play"),
list("rain"    ,      68     ,    80    , 'calm' , "Play"),
list("rain"    ,      65     ,    70    , "wind"  , "Don't Play"),
list("overcast",      64     ,    65    , "wind"  , "Play"),
list("sunny"   ,      72     ,    95    , 'calm' , "Don't Play"),
list("sunny"   ,      69     ,    70    , 'calm' , "Play"),
list("rain"    ,      75     ,    80    , 'calm' , "Play"),
list("sunny"   ,      75     ,    70    , "wind"  , "Play"),
list("overcast",      72     ,    90    , "wind"  , "Play"),
list("overcast",      81     ,    75    , 'calm' , "Play"),
list("rain"    ,      71     ,    80    , "wind"  , "Don't Play")))
names(data) <- c('outlook', 'temperature', 'humidity', 'windy', 'play')
data$temperature <- as.numeric(data$temperature)
data$humidity <- as.numeric(data$humidity)
data$windy <- as.factor(unlist(data$windy))
data$outlook <- as.factor(unlist(data$outlook))
data$play <- as.factor(unlist(data$play))
data$temperature <- cut(data$temperature, breaks = c(0, 75, max(data$temperature)))
data$humidity <- cut(data$humidity, breaks = c(0, 75, max(data$humidity)))
decision.tree <- ID3(data[-1, ], 'play')
predict_ID3(data[1,], decision.tree)
View(x)
View(test)
View(train)
View(secom)
View(test)
View(test)
View(train)
modelID3 <- ID3(train,'y')
pID3 <- predict_ID3(train, modelID3)
pID3 <- predict_ID3(test[1,], modelID3)
getOption('expressions')
options(expressions = 500000)
pID3 <- predict_ID3(test[1,], modelID3)
options(expressions = 5000000)
install.packages("RWeka")
library(RWeka)
install.packages("rJava")
install.packages("RWeka")
library(rJava)
install.packages("RWeka")
install.packages("rJava")
install.packages("RWeka")
library(rJava)
install.packages("rJava", type='source')
install.packages("rJava", type = "source")
library("rJava", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
detach("package:rJava", unload=TRUE)
library("rJava", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
detach("package:rJava", unload=TRUE)
install.packages("rJava",type='source')
library("rJava", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
install.packages("rJava")
library("rJava", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
install.packages("RWeka")
install.packages("rJava",type='source')
library(rJava)
library(rJava)
library("rJava", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
